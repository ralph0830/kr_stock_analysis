"""
Chatbot LLM Client
Gemini API ê¸°ë°˜ LLM í´ë¼ì´ì–¸íŠ¸
"""

import logging
import os
from typing import List, Dict, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)


class LLMInitializationError(Exception):
    """LLM ì´ˆê¸°í™” ì‹¤íŒ¨ ì˜ˆì™¸"""
    pass


@dataclass
class LLMResponse:
    """LLM ì‘ë‹µ ëª¨ë¸"""
    reply: str
    suggestions: List[str]
    usage: Optional[Dict] = None


class LLMClient:
    """
    LLM í´ë¼ì´ì–¸íŠ¸

    Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    API í‚¤ê°€ ì—†ìœ¼ë©´ Mock ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.
    """

    # Mock ì‘ë‹µ í…œí”Œë¦¿
    MOCK_RESPONSES = {
        "ì¶”ì²œ": "VCP Aë“±ê¸‰ ì¢…ëª©ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤. í˜„ì¬ ì‚¼ì„±ì „ì(005930)ê°€ VCP íŒ¨í„´ì„ í˜•ì„±í•˜ê³  ìˆì–´ ë§¤ìˆ˜ ê¸°íšŒë¡œ ë³´ì…ë‹ˆë‹¤. SmartMoney íë¦„ë„ ê¸ì •ì ì…ë‹ˆë‹¤.",
        "ì‹œì¥": "í˜„ì¬ Market Gate ìƒíƒœëŠ” YELLOWì…ë‹ˆë‹¤. KOSPIëŠ” íš¡ë³´ ì¤‘ì´ë©°, KOSDAQì€ ì•½ì„¸ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. ì™¸êµ­ì¸ê³¼ ê¸°ê´€ì˜ ìˆ˜ê¸‰ ë³€í™”ë¥¼ ì£¼ì˜ ê¹Šê²Œ ì‚´í´ë³´ì„¸ìš”.",
        "ì‚¼ì„±ì „ì": "ì‚¼ì„±ì „ì(005930)ëŠ” í˜„ì¬ VCP Aë“±ê¸‰ì…ë‹ˆë‹¤. ì¢…ê°€ë² íŒ… ì ìˆ˜ëŠ” 85ì ìœ¼ë¡œ ë§¤ìˆ˜ ì¶”ì²œì…ë‹ˆë‹¤. ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜ê°€ ì§€ì†ë˜ê³  ìˆì–´ ì¶”ê°€ ìƒìŠ¹ ì—¬ë ¥ì´ ìˆìŠµë‹ˆë‹¤.",
        "default": "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì¢…ëª©ì— ëŒ€í•œ ë¶„ì„ ì •ë³´ë¥¼ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤. VCP Aë“±ê¸‰ ì¢…ëª©ì´ë‚˜ Market Gate ìƒíƒœë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.",
    }

    def __init__(self, api_key: Optional[str] = None):
        """
        LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”

        Args:
            api_key: Gemini API í‚¤ (Noneì´ë©´ í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©)
        """
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        self._use_mock = False
        self._client = None

        # API í‚¤ê°€ ì—†ê±°ë‚˜ í…ŒìŠ¤íŠ¸ìš© í‚¤ë©´ Mock ëª¨ë“œ
        if not self.api_key or self.api_key == "test-key":
            self._use_mock = True
            self._client = None
            logger.info("ğŸ¤– LLM Mock mode enabled")
            return

        try:
            import google.generativeai as genai
            genai.configure(api_key=self.api_key)
            self._client = genai.GenerativeModel("gemini-3-flash-preview")
            logger.info("âœ… Gemini LLM initialized")
        except ImportError as e:
            logger.warning(f"google-generativeai not installed: {e}, falling back to mock mode")
            self._use_mock = True
        except Exception as e:
            logger.warning(f"Gemini API initialization failed: {e}, falling back to mock mode")
            self._use_mock = True

    def generate_reply(
        self,
        prompt: str,
        conversation_history: Optional[List[Dict]] = None
    ) -> LLMResponse:
        """
        LLM ë‹µë³€ ìƒì„±

        Args:
            prompt: í”„ë¡¬í”„íŠ¸
            conversation_history: ëŒ€í™” ê¸°ë¡

        Returns:
            LLM ì‘ë‹µ
        """
        # Mock ëª¨ë“œ
        if self._use_mock:
            return self._generate_mock_reply(prompt)

        try:
            # Gemini API í˜¸ì¶œ
            response = self._client.generate_content(prompt)
            reply_text = response.text.strip()

            # ì¶”ì²œ ì§ˆë¬¸ ì¶”ì¶œ
            suggestions = self._extract_suggestions(reply_text)

            return LLMResponse(
                reply=reply_text,
                suggestions=suggestions,
                usage=None,
            )

        except Exception as e:
            logger.error(f"âŒ LLM generation failed: {e}, falling back to mock")
            # API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ Mockìœ¼ë¡œ fallback
            return self._generate_mock_reply(prompt)

    def _generate_mock_reply(self, prompt: str) -> LLMResponse:
        """
        Mock ì‘ë‹µ ìƒì„±

        Args:
            prompt: í”„ë¡¬í”„íŠ¸

        Returns:
            Mock LLM ì‘ë‹µ
        """
        # ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        question = self._extract_question_from_prompt(prompt)

        # ì ì ˆí•œ Mock ì‘ë‹µ ì„ íƒ
        reply = self._get_mock_response(question)

        # ì¶”ì²œ ì§ˆë¬¸ ìƒì„±
        suggestions = self._extract_suggestions(reply)

        return LLMResponse(
            reply=reply,
            suggestions=suggestions,
            usage=None,
        )

    def _get_mock_response(self, question: str) -> str:
        """ì§ˆë¬¸ì— ë§ëŠ” Mock ì‘ë‹µ ë°˜í™˜"""
        if "ì¶”ì²œ" in question:
            return self.MOCK_RESPONSES["ì¶”ì²œ"]
        elif "ì‹œì¥" in question or "ìƒí™©" in question or "ìƒíƒœ" in question:
            return self.MOCK_RESPONSES["ì‹œì¥"]
        elif "ì‚¼ì„±ì „ì" in question or "005930" in question:
            return self.MOCK_RESPONSES["ì‚¼ì„±ì „ì"]
        else:
            return self.MOCK_RESPONSES["default"]

    def _extract_question_from_prompt(self, prompt: str) -> str:
        """
        í”„ë¡¬í”„íŠ¸ì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ì¶œ

        Args:
            prompt: ì „ì²´ í”„ë¡¬í”„íŠ¸

        Returns:
            ì¶”ì¶œëœ ì§ˆë¬¸ í…ìŠ¤íŠ¸
        """
        # "## ì‚¬ìš©ì ì§ˆë¬¸" ì„¹ì…˜ ì¶”ì¶œ
        if "## ì‚¬ìš©ì ì§ˆë¬¸" in prompt:
            parts = prompt.split("## ì‚¬ìš©ì ì§ˆë¬¸")
            if len(parts) > 1:
                question = parts[1].strip()
                # ë‹¤ë¥¸ ì„¹ì…˜ì´ ìˆìœ¼ë©´ ì˜ë¼ë‚´ê¸°
                for marker in ["## ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸", "## ì¢…ëª© ë°ì´í„°", "## ì‚¬ìš©ì ì§ˆë¬¸"]:
                    if marker in question and marker != "## ì‚¬ìš©ì ì§ˆë¬¸":
                        question = question.split(marker)[0].strip()
                return question

        # ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ì „ì²´ ë°˜í™˜
        return prompt.strip()

    def _extract_suggestions(self, reply_text: str) -> List[str]:
        """ì‘ë‹µì—ì„œ ì¶”ì²œ ì§ˆë¬¸ ì¶”ì¶œ"""
        suggestions = []

        if "VCP" in reply_text or "íŒ¨í„´" in reply_text:
            suggestions.append("VCP ì‹œê·¸ë„ í™•ì¸")

        if "ì‚¼ì„±ì „ì" in reply_text or "005930" in reply_text:
            suggestions.append("ì‚¼ì„±ì „ì ìˆ˜ê¸‰")

        if "Market Gate" in reply_text or "ì‹œì¥" in reply_text:
            suggestions.append("KOSPI í˜„í™©")

        if "ì¶”ì²œ" in reply_text:
            suggestions.append("ì¶”ì²œ ì¢…ëª© ë”ë³´ê¸°")

        if not suggestions:
            suggestions = ["ì˜¤ëŠ˜ì˜ ì¶”ì²œì¢…ëª©", "Market Gate ìƒíƒœ", "VCP ì‹œê·¸ë„ í™•ì¸"]

        return suggestions[:3]

    def is_available(self) -> bool:
        """LLM ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        # Mock ëª¨ë“œê°€ ì•„ë‹ˆê³  í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ëœ ê²½ìš°
        return not self._use_mock and self._client is not None


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_llm_client: Optional[LLMClient] = None


def get_llm_client() -> Optional[LLMClient]:
    """LLM í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _llm_client
    if _llm_client is None:
        try:
            _llm_client = LLMClient()
        except Exception:
            # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ Mock ëª¨ë“œë¡œ ìƒì„±
            _llm_client = LLMClient(api_key=None)
    return _llm_client
