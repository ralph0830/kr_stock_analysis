"""
News Collector
ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° - ë„¤ì´ë²„/ë‹¤ìŒ/ì—°í•©ë‰´ìŠ¤ í¬ë¡¤ë§
"""

import os
import logging
import asyncio
from typing import List, Dict, Any, Optional
from datetime import date, datetime, timedelta
from dataclasses import dataclass
import time

import requests
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)


@dataclass
class NewsArticle:
    """ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° í´ë˜ìŠ¤"""
    title: str
    content: str
    source: str  # ì–¸ë¡ ì‚¬
    url: str
    published_at: datetime
    ticker: str  # ì¢…ëª©ì½”ë“œ


class NewsCollector:
    """
    ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°

    ë„¤ì´ë²„/ë‹¤ìŒ/ì—°í•©ë‰´ìŠ¤ì—ì„œ ì¢…ëª© ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘
    """

    # ìš”ì²­ ê°„ê²© (robots.txt ì¤€ìˆ˜)
    REQUEST_INTERVAL = 1.0  # 1ì´ˆ

    # User-Agent
    USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"

    # RSS í”¼ë“œ URL
    NAVER_FINANCE_RSS = "https://finance.naver.com/news/news_list.naver?mode=RSS"
    DAUM_FINANCE_RSS = "https://news.daum.net/breakingnews/economic"
    YONHAP_ECONOMY_RSS = "https://www.yonhapnewstv.co.kr/category/economy/feed"

    def __init__(self):
        """NewsCollector ì´ˆê¸°í™”"""
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": self.USER_AGENT})
        self._last_request_time = 0

    def _wait_for_rate_limit(self):
        """ìš”ì²­ ê°„ê²© ì¤€ìˆ˜ (rate limiting)"""
        current_time = time.time()
        elapsed = current_time - self._last_request_time

        if elapsed < self.REQUEST_INTERVAL:
            sleep_time = self.REQUEST_INTERVAL - elapsed
            logger.debug(f"Rate limiting: sleeping {sleep_time:.2f}s")
            time.sleep(sleep_time)

        self._last_request_time = time.time()

    def fetch_stock_news(
        self,
        ticker: str,
        days: int = 7,
        max_articles: int = 50,
    ) -> List[NewsArticle]:
        """
        ì¢…ëª© ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘

        Args:
            ticker: ì¢…ëª©ì½”ë“œ (ì˜ˆ: "005930" for ì‚¼ì„±ì „ì)
            days: ìˆ˜ì§‘í•  ë‚ ì§œ ë²”ìœ„ (ê¸°ë³¸ 7ì¼)
            max_articles: ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ 50ê±´)

        Returns:
            ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ“° {ticker} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ (ìµœê·¼ {days}ì¼, ìµœëŒ€ {max_articles}ê±´)")

        # ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ (ì£¼ìš” ì†ŒìŠ¤)
        articles = self._fetch_naver_news(ticker, days, max_articles)

        # ë¶€ì¡±í•˜ë©´ ë‹¤ìŒ ë‰´ìŠ¤ ìˆ˜ì§‘
        if len(articles) < max_articles:
            additional = self._fetch_daum_news(ticker, days, max_articles - len(articles))
            articles.extend(additional)

        # ë¶€ì¡±í•˜ë©´ ì—°í•©ë‰´ìŠ¤ ìˆ˜ì§‘
        if len(articles) < max_articles:
            additional = self._fetch_yonhap_news(ticker, days, max_articles - len(articles))
            articles.extend(additional)

        # ë‚ ì§œìˆœ ì •ë ¬ ë° ì¤‘ë³µ ì œê±°
        seen_urls = set()
        unique_articles = []
        for article in sorted(articles, key=lambda x: x.published_at, reverse=True):
            if article.url not in seen_urls:
                seen_urls.add(article.url)
                unique_articles.append(article)

            if len(unique_articles) >= max_articles:
                break

        logger.info(f"âœ… {ticker} ë‰´ìŠ¤ {len(unique_articles)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
        return unique_articles[:max_articles]

    def _fetch_naver_news(
        self,
        ticker: str,
        days: int,
        max_articles: int,
    ) -> List[NewsArticle]:
        """
        ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘

        ë„¤ì´ë²„ ê¸ˆìœµ ì¢…ëª© í˜ì´ì§€ ë‰´ìŠ¤ í¬ë¡¤ë§
        """
        articles = []

        try:
            # ë„¤ì´ë²„ ê¸ˆìœµ ì¢…ëª© ë‰´ìŠ¤ URL
            url = f"https://finance.naver.com/item/news_news.nhn?code={ticker}&page=1"

            self._wait_for_rate_limit()
            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")

            # ë‰´ìŠ¤ ëª©ë¡ ì¶”ì¶œ
            news_list = soup.select("table.type5 tr")

            for row in news_list:
                try:
                    # ì œëª© ë° ë§í¬
                    title_element = row.select_one("td.title a")
                    if not title_element:
                        continue

                    title = title_element.get_text(strip=True)
                    article_url = title_element.get("href", "")

                    # ì •ë³´ì› ë° ë‚ ì§œ
                    info_element = row.select_one("td.info")
                    if not info_element:
                        continue

                    info_text = info_element.get_text(strip=True)
                    parts = info_text.split()

                    if len(parts) < 2:
                        continue

                    source = parts[0]
                    date_str = parts[1]

                    # ë‚ ì§œ íŒŒì‹±
                    published_at = self._parse_naver_date(date_str)

                    # ë‚ ì§œ ë²”ìœ„ í™•ì¸
                    if (datetime.now() - published_at).days > days:
                        continue

                    # ë³¸ë¬¸ ìˆ˜ì§‘ (ë³„ë„ ìš”ì²­)
                    content = self._fetch_article_content(article_url)

                    articles.append(NewsArticle(
                        title=title,
                        content=content,
                        source=source,
                        url=article_url,
                        published_at=published_at,
                        ticker=ticker,
                    ))

                    if len(articles) >= max_articles:
                        break

                except Exception as e:
                    logger.debug(f"ë„¤ì´ë²„ ë‰´ìŠ¤ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue

            logger.debug(f"ë„¤ì´ë²„ ë‰´ìŠ¤ {len(articles)}ê±´ ìˆ˜ì§‘")

        except Exception as e:
            logger.error(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        return articles

    def _fetch_daum_news(
        self,
        ticker: str,
        days: int,
        max_articles: int,
    ) -> List[NewsArticle]:
        """
        ë‹¤ìŒ ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘

        ë‹¤ìŒ ê¸ˆìœµ ì¢…ëª© í˜ì´ì§€ ë‰´ìŠ¤ í¬ë¡¤ë§
        """
        # TODO: ë‹¤ìŒ ë‰´ìŠ¤ í¬ë¡¤ë§ êµ¬í˜„
        logger.debug("ë‹¤ìŒ ë‰´ìŠ¤ ìˆ˜ì§‘ (ì•„ì§ êµ¬í˜„ë˜ì§€ ì•ŠìŒ)")
        return []

    def _fetch_yonhap_news(
        self,
        ticker: str,
        days: int,
        max_articles: int,
    ) -> List[NewsArticle]:
        """
        ì—°í•©ë‰´ìŠ¤ ê²½ì œ ë‰´ìŠ¤ ìˆ˜ì§‘

        ì—°í•©ë‰´ìŠ¤ RSS í”¼ë“œ íŒŒì‹±
        """
        # TODO: ì—°í•©ë‰´ìŠ¤ RSS êµ¬í˜„
        logger.debug("ì—°í•©ë‰´ìŠ¤ ìˆ˜ì§‘ (ì•„ì§ êµ¬í˜„ë˜ì§€ ì•ŠìŒ)")
        return []

    def _fetch_article_content(self, url: str) -> str:
        """
        ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘

        Args:
            url: ê¸°ì‚¬ URL

        Returns:
            ê¸°ì‚¬ ë³¸ë¬¸ í…ìŠ¤íŠ¸
        """
        try:
            self._wait_for_rate_limit()
            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")

            # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ
            content_element = soup.select_one("div.articleBody")
            if content_element:
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for element in content_element.select("script, style, .ad"):
                    element.decompose()

                content = content_element.get_text(separator="\n", strip=True)
                return content[:5000]  # ìµœëŒ€ 5000ì ì œí•œ

        except Exception as e:
            logger.debug(f"ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨ ({url}): {e}")

        return ""

    def _parse_naver_date(self, date_str: str) -> datetime:
        """
        ë„¤ì´ë²„ ë‰´ìŠ¤ ë‚ ì§œ íŒŒì‹±

        Args:
            date_str: ë‚ ì§œ ë¬¸ìì—´ (ì˜ˆ: "2024.01.15 14:30")

        Returns:
            datetime ê°ì²´
        """
        try:
            # ê³µë°± ì œê±°
            date_str = date_str.strip()

            # ì˜¤ëŠ˜/ì–´ì œ í‘œí˜„ ì²˜ë¦¬
            if date_str.startswith("ì˜¤ëŠ˜"):
                today = datetime.now().date()
                time_part = date_str.split()[1] if len(date_str.split()) > 1 else "00:00"
                hour, minute = map(int, time_part.split(":"))
                return datetime.combine(today, datetime.min.time()).replace(hour=hour, minute=minute)

            elif date_str.startswith("ì–´ì œ"):
                yesterday = datetime.now().date() - timedelta(days=1)
                time_part = date_str.split()[1] if len(date_str.split()) > 1 else "00:00"
                hour, minute = map(int, time_part.split(":"))
                return datetime.combine(yesterday, datetime.min.time()).replace(hour=hour, minute=minute)

            # ì¼ë°˜ ë‚ ì§œ í¬ë§· (2024.01.15 14:30)
            date_str = date_str.replace(".", "").replace(":", "")
            return datetime.strptime(date_str, "%Y%m%d %H%M")

        except Exception as e:
            logger.debug(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ({date_str}): {e}")
            return datetime.now()

    def to_dict(self, article: NewsArticle) -> Dict[str, Any]:
        """
        NewsArticleì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜

        Args:
            article: NewsArticle ê°ì²´

        Returns:
            ë”•ì…”ë„ˆë¦¬
        """
        return {
            "title": article.title,
            "content": article.content,
            "source": article.source,
            "url": article.url,
            "published_at": article.published_at.isoformat(),
            "ticker": article.ticker,
        }

    def to_dict_list(self, articles: List[NewsArticle]) -> List[Dict[str, Any]]:
        """
        NewsArticle ë¦¬ìŠ¤íŠ¸ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

        Args:
            articles: NewsArticle ë¦¬ìŠ¤íŠ¸

        Returns:
            ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        return [self.to_dict(article) for article in articles]
