"""
News Collector
ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° - ë„¤ì´ë²„/ë‹¤ìŒ/ì—°í•©ë‰´ìŠ¤ í¬ë¡¤ë§
"""

import logging
import re
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass
import time

import requests
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)


@dataclass
class NewsArticle:
    """ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° í´ë˜ìŠ¤"""
    title: str
    content: str
    source: str  # ì–¸ë¡ ì‚¬
    url: str
    published_at: datetime
    ticker: str  # ì¢…ëª©ì½”ë“œ


class NewsCollector:
    """
    ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°

    ë„¤ì´ë²„/ë‹¤ìŒ/ì—°í•©ë‰´ìŠ¤ì—ì„œ ì¢…ëª© ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘
    """

    # ìš”ì²­ ê°„ê²© (robots.txt ì¤€ìˆ˜)
    REQUEST_INTERVAL = 1.0  # 1ì´ˆ

    # User-Agent
    USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"

    # RSS í”¼ë“œ URL
    NAVER_FINANCE_RSS = "https://finance.naver.com/news/news_list.naver?mode=RSS"
    DAUM_FINANCE_RSS = "https://news.daum.net/breakingnews/economic"
    YONHAP_ECONOMY_RSS = "https://www.yonhapnewstv.co.kr/category/economy/feed"

    # ë„¤ì´ë²„ ë‰´ìŠ¤ URL íŒ¨í„´ (Phase 1: ì‹¤ì œ ê¸°ì‚¬ URL ì¶”ì¶œ)
    NAVER_NEWS_URL_PATTERN = re.compile(
        r'https://n\.news\.naver\.com/mnews/article/(\d+)/(\d+)'
    )

    def __init__(self):
        """NewsCollector ì´ˆê¸°í™”"""
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": self.USER_AGENT})
        self._last_request_time = 0

    def _wait_for_rate_limit(self):
        """ìš”ì²­ ê°„ê²© ì¤€ìˆ˜ (rate limiting)"""
        current_time = time.time()
        elapsed = current_time - self._last_request_time

        if elapsed < self.REQUEST_INTERVAL:
            sleep_time = self.REQUEST_INTERVAL - elapsed
            logger.debug(f"Rate limiting: sleeping {sleep_time:.2f}s")
            time.sleep(sleep_time)

        self._last_request_time = time.time()

    def fetch_stock_news(
        self,
        ticker: str,
        days: int = 7,
        max_articles: int = 50,
    ) -> List[NewsArticle]:
        """
        ì¢…ëª© ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘

        Args:
            ticker: ì¢…ëª©ì½”ë“œ (ì˜ˆ: "005930" for ì‚¼ì„±ì „ì)
            days: ìˆ˜ì§‘í•  ë‚ ì§œ ë²”ìœ„ (ê¸°ë³¸ 7ì¼)
            max_articles: ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ 50ê±´)

        Returns:
            ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ“° {ticker} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ (ìµœê·¼ {days}ì¼, ìµœëŒ€ {max_articles}ê±´)")

        # ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ (ì£¼ìš” ì†ŒìŠ¤)
        articles = self._fetch_naver_news(ticker, days, max_articles)

        # ë¶€ì¡±í•˜ë©´ ë‹¤ìŒ ë‰´ìŠ¤ ìˆ˜ì§‘
        if len(articles) < max_articles:
            additional = self._fetch_daum_news(ticker, days, max_articles - len(articles))
            articles.extend(additional)

        # ë¶€ì¡±í•˜ë©´ ì—°í•©ë‰´ìŠ¤ ìˆ˜ì§‘
        if len(articles) < max_articles:
            additional = self._fetch_yonhap_news(ticker, days, max_articles - len(articles))
            articles.extend(additional)

        # ë‚ ì§œìˆœ ì •ë ¬ ë° ì¤‘ë³µ ì œê±°
        seen_urls = set()
        unique_articles = []
        for article in sorted(articles, key=lambda x: x.published_at, reverse=True):
            if article.url not in seen_urls:
                seen_urls.add(article.url)
                unique_articles.append(article)

            if len(unique_articles) >= max_articles:
                break

        logger.info(f"âœ… {ticker} ë‰´ìŠ¤ {len(unique_articles)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
        return unique_articles[:max_articles]

    def _fetch_naver_news(
        self,
        ticker: str,
        days: int,
        max_articles: int,
    ) -> List[NewsArticle]:
        """
        ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘

        ë„¤ì´ë²„ ê¸ˆìœµ ì¢…ëª© í˜ì´ì§€ ë‰´ìŠ¤ í¬ë¡¤ë§
        """
        articles = []

        try:
            # ë„¤ì´ë²„ ê¸ˆìœµ ì¢…ëª© ë‰´ìŠ¤ URL
            url = f"https://finance.naver.com/item/news_news.nhn?code={ticker}&page=1"

            self._wait_for_rate_limit()
            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")

            # ë‰´ìŠ¤ ëª©ë¡ ì¶”ì¶œ
            news_list = soup.select("table.type5 tr")

            for row in news_list:
                try:
                    # ì œëª© ë° ë§í¬
                    title_element = row.select_one("td.title a")
                    if not title_element:
                        continue

                    title = title_element.get_text(strip=True)
                    article_url = title_element.get("href", "")

                    # ì •ë³´ì› ë° ë‚ ì§œ
                    info_element = row.select_one("td.info")
                    if not info_element:
                        continue

                    info_text = info_element.get_text(strip=True)
                    parts = info_text.split()

                    if len(parts) < 2:
                        continue

                    source = parts[0]
                    date_str = parts[1]

                    # ë‚ ì§œ íŒŒì‹±
                    published_at = self._parse_naver_date(date_str)

                    # ë‚ ì§œ ë²”ìœ„ í™•ì¸
                    if (datetime.now() - published_at).days > days:
                        continue

                    # ë³¸ë¬¸ ìˆ˜ì§‘ (ë³„ë„ ìš”ì²­)
                    content = self._fetch_article_content(article_url)

                    articles.append(NewsArticle(
                        title=title,
                        content=content,
                        source=source,
                        url=article_url,
                        published_at=published_at,
                        ticker=ticker,
                    ))

                    if len(articles) >= max_articles:
                        break

                except Exception as e:
                    logger.debug(f"ë„¤ì´ë²„ ë‰´ìŠ¤ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue

            logger.debug(f"ë„¤ì´ë²„ ë‰´ìŠ¤ {len(articles)}ê±´ ìˆ˜ì§‘")

        except Exception as e:
            logger.error(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        return articles

    def _fetch_daum_news(
        self,
        ticker: str,
        days: int,
        max_articles: int,
    ) -> List[NewsArticle]:
        """
        ë‹¤ìŒ ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘

        ë‹¤ìŒ ê¸ˆìœµ ì¢…ëª© í˜ì´ì§€ ë‰´ìŠ¤ í¬ë¡¤ë§
        """
        # TODO: ë‹¤ìŒ ë‰´ìŠ¤ í¬ë¡¤ë§ êµ¬í˜„
        logger.debug("ë‹¤ìŒ ë‰´ìŠ¤ ìˆ˜ì§‘ (ì•„ì§ êµ¬í˜„ë˜ì§€ ì•ŠìŒ)")
        return []

    def _fetch_yonhap_news(
        self,
        ticker: str,
        days: int,
        max_articles: int,
    ) -> List[NewsArticle]:
        """
        ì—°í•©ë‰´ìŠ¤ ê²½ì œ ë‰´ìŠ¤ ìˆ˜ì§‘

        ì—°í•©ë‰´ìŠ¤ RSS í”¼ë“œ íŒŒì‹±
        """
        # TODO: ì—°í•©ë‰´ìŠ¤ RSS êµ¬í˜„
        logger.debug("ì—°í•©ë‰´ìŠ¤ ìˆ˜ì§‘ (ì•„ì§ êµ¬í˜„ë˜ì§€ ì•ŠìŒ)")
        return []

    def _fetch_article_content(self, url: str) -> str:
        """
        ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘

        Args:
            url: ê¸°ì‚¬ URL

        Returns:
            ê¸°ì‚¬ ë³¸ë¬¸ í…ìŠ¤íŠ¸
        """
        try:
            self._wait_for_rate_limit()
            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")

            # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ
            content_element = soup.select_one("div.articleBody")
            if content_element:
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for element in content_element.select("script, style, .ad"):
                    element.decompose()

                content = content_element.get_text(separator="\n", strip=True)
                return content[:5000]  # ìµœëŒ€ 5000ì ì œí•œ

        except Exception as e:
            logger.debug(f"ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨ ({url}): {e}")

        return ""

    def _parse_naver_date(self, date_str: str) -> datetime:
        """
        ë„¤ì´ë²„ ë‰´ìŠ¤ ë‚ ì§œ íŒŒì‹±

        Args:
            date_str: ë‚ ì§œ ë¬¸ìì—´ (ì˜ˆ: "2024.01.15 14:30")

        Returns:
            datetime ê°ì²´
        """
        try:
            # ê³µë°± ì œê±°
            date_str = date_str.strip()

            # ì˜¤ëŠ˜/ì–´ì œ í‘œí˜„ ì²˜ë¦¬
            if date_str.startswith("ì˜¤ëŠ˜"):
                today = datetime.now().date()
                time_part = date_str.split()[1] if len(date_str.split()) > 1 else "00:00"
                hour, minute = map(int, time_part.split(":"))
                return datetime.combine(today, datetime.min.time()).replace(hour=hour, minute=minute)

            elif date_str.startswith("ì–´ì œ"):
                yesterday = datetime.now().date() - timedelta(days=1)
                time_part = date_str.split()[1] if len(date_str.split()) > 1 else "00:00"
                hour, minute = map(int, time_part.split(":"))
                return datetime.combine(yesterday, datetime.min.time()).replace(hour=hour, minute=minute)

            # ì¼ë°˜ ë‚ ì§œ í¬ë§· (2024.01.15 14:30)
            date_str = date_str.replace(".", "").replace(":", "")
            return datetime.strptime(date_str, "%Y%m%d %H%M")

        except Exception as e:
            logger.debug(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ({date_str}): {e}")
            return datetime.now()

    # ==========================================================================
    # Phase 1: ë„¤ì´ë²„ ë‰´ìŠ¤ ì‹¤ì œ URL ì¶”ì¶œ ë©”ì„œë“œ
    # ==========================================================================

    def _is_valid_naver_news_url(self, url: str) -> bool:
        """
        ë„¤ì´ë²„ ë‰´ìŠ¤ ì‹¤ì œ ê¸°ì‚¬ URLì¸ì§€ í™•ì¸

        Args:
            url: ê²€ì‚¬í•  URL

        Returns:
            ì˜¬ë°”ë¥¸ ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ URLì´ë©´ True
        """
        if not url:
            return False

        # ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ URL íŒ¨í„´ í™•ì¸
        # https://n.news.naver.com/mnews/article/{office_id}/{article_id}
        return bool(self.NAVER_NEWS_URL_PATTERN.match(url))

    def _parse_naver_news_url(self, url: str) -> Tuple[str, str]:
        """
        ë„¤ì´ë²„ ë‰´ìŠ¤ URLì—ì„œ office_idì™€ article_id ì¶”ì¶œ

        Args:
            url: ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ URL

        Returns:
            (office_id, article_id) íŠœí”Œ

        Raises:
            ValueError: ì˜¬ë°”ë¥´ì§€ ì•Šì€ URLì¸ ê²½ìš°
        """
        match = self.NAVER_NEWS_URL_PATTERN.match(url)
        if not match:
            raise ValueError(f"ì˜¬ë°”ë¥´ì§€ ì•Šì€ ë„¤ì´ë²„ ë‰´ìŠ¤ URL: {url}")

        office_id = match.group(1)
        article_id = match.group(2)
        return office_id, article_id

    def _extract_naver_news_urls(
        self,
        query: str,
        max_results: int = 10,
    ) -> List[str]:
        """
        ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ì—ì„œ ì‹¤ì œ ê¸°ì‚¬ URL ì¶”ì¶œ

        Args:
            query: ê²€ìƒ‰ì–´ (ì¢…ëª©ëª… ë˜ëŠ” í‹°ì»¤)
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜

        Returns:
            ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ URL ë¦¬ìŠ¤íŠ¸ (ì¤‘ë³µ ì œê±°ë¨)
        """
        urls = []

        try:
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL
            search_url = (
                f"https://search.naver.com/search.naver"
                f"?where=news&sm=tab_pge&query={query}&sort=1&start=1"
            )

            self._wait_for_rate_limit()
            response = self.session.get(search_url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")

            # ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë§í¬ ì¶”ì¶œ
            for link in soup.find_all("a", href=True):
                href = link["href"]

                # ì˜¬ë°”ë¥¸ ë„¤ì´ë²„ ë‰´ìŠ¤ URLì¸ì§€ í™•ì¸
                if self._is_valid_naver_news_url(href):
                    if href not in urls:  # ì¤‘ë³µ ì œê±°
                        urls.append(href)

                    if len(urls) >= max_results:
                        break

            logger.debug(f"ê²€ìƒ‰ '{query}'ì—ì„œ {len(urls)}ê°œì˜ ë‰´ìŠ¤ URL ì¶”ì¶œ")

        except Exception as e:
            logger.error(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨ ({query}): {e}")

        return urls

    def _fetch_naver_news_with_urls(
        self,
        query: str,
        days: int = 7,
        max_articles: int = 10,
    ) -> List[Dict[str, Any]]:
        """
        ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ìœ¼ë¡œ ê¸°ì‚¬ ìˆ˜ì§‘ (ì‹¤ì œ URL í¬í•¨)

        Args:
            query: ê²€ìƒ‰ì–´ (ì¢…ëª©ëª… ë˜ëŠ” í‹°ì»¤)
            days: ìˆ˜ì§‘í•  ë‚ ì§œ ë²”ìœ„
            max_articles: ìµœëŒ€ ê¸°ì‚¬ ìˆ˜

        Returns:
            ê¸°ì‚¬ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
            [
                {
                    "title": "ê¸°ì‚¬ ì œëª©",
                    "url": "https://n.news.naver.com/mnews/article/...",
                    "source": "ì–¸ë¡ ì‚¬",
                    "published_at": "2024-01-30T10:00:00",
                    "content": "ê¸°ì‚¬ ë³¸ë¬¸"
                }
            ]
        """
        articles = []

        try:
            # URL ì¶”ì¶œ
            urls = self._extract_naver_news_urls(query, max_results=max_articles)

            for url in urls:
                try:
                    # ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    article_data = self._fetch_article_details(url)
                    if not article_data:
                        continue

                    # ë‚ ì§œ ë²”ìœ„ í™•ì¸
                    published_at = article_data.get("published_at")
                    if published_at and (datetime.now() - published_at).days > days:
                        continue

                    articles.append(article_data)

                    if len(articles) >= max_articles:
                        break

                except Exception as e:
                    logger.debug(f"ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨ ({url}): {e}")
                    continue

            # ìµœì‹  ë‰´ìŠ¤ ìš°ì„  ì •ë ¬
            articles.sort(
                key=lambda x: x.get("published_at", datetime.min),
                reverse=True
            )

            logger.debug(f"ì´ {len(articles)}ê±´ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        return articles

    def _fetch_article_details(self, url: str) -> Optional[Dict[str, Any]]:
        """
        ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘

        Args:
            url: ê¸°ì‚¬ URL

        Returns:
            ê¸°ì‚¬ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
        """
        try:
            self._wait_for_rate_limit()
            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")

            # ì œëª© ì¶”ì¶œ
            title_elem = soup.select_one("h2#title_area") or soup.select_one("h3.title") or soup.select_one("meta[property='og:title']")
            title = ""
            if title_elem:
                if title_elem.name == "meta":
                    title = title_elem.get("content", "")
                else:
                    title = title_elem.get_text(strip=True)

            # ì–¸ë¡ ì‚¬ ì¶”ì¶œ
            source_elem = soup.select_one(".media_end_head_top_link_text") or soup.select_one(".press_name") or soup.select_one("meta[property='og:article:author']")
            source = ""
            if source_elem:
                if source_elem.name == "meta":
                    source = source_elem.get("content", "")
                else:
                    source = source_elem.get_text(strip=True)

            # ë‚ ì§œ ì¶”ì¶œ
            date_elem = soup.select_one(".media_end_head_info_datestamp_bunch") or soup.select_one(".date") or soup.select_one("meta[property='article:published_time']")
            published_at = None
            if date_elem:
                if date_elem.name == "meta":
                    date_str = date_elem.get("content", "")
                    try:
                        published_at = datetime.fromisoformat(date_str.replace("Z", "+00:00"))
                    except:
                        pass
                else:
                    date_text = date_elem.get_text(strip=True)
                    # ì—¬ëŸ¬ ë‚ ì§œ í¬ë§· ì‹œë„
                    published_at = self._parse_news_date(date_text)

            if not published_at:
                published_at = datetime.now()

            # ë³¸ë¬¸ ì¶”ì¶œ
            content_elem = soup.select_one("#newsct_article") or soup.select_one(".article_body") or soup.select_one("div#articleBody")
            content = ""
            if content_elem:
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for elem in content_elem.select("script, style, .ad, .caption"):
                    elem.decompose()

                content = content_elem.get_text(separator="\n", strip=True)[:5000]

            return {
                "title": title,
                "url": url,
                "source": source,
                "published_at": published_at.isoformat() if isinstance(published_at, datetime) else published_at,
                "content": content,
            }

        except Exception as e:
            logger.debug(f"ê¸°ì‚¬ ìƒì„¸ ìˆ˜ì§‘ ì‹¤íŒ¨ ({url}): {e}")
            return None

    def _parse_news_date(self, date_str: str) -> Optional[datetime]:
        """
        ë‹¤ì–‘í•œ ë‰´ìŠ¤ ë‚ ì§œ í¬ë§· íŒŒì‹±

        Args:
            date_str: ë‚ ì§œ ë¬¸ìì—´

        Returns:
            datetime ê°ì²´ ë˜ëŠ” None
        """
        try:
            # ë‹¤ì–‘í•œ í¬ë§· ì‹œë„
            formats = [
                "%Y.%m.%d. %H:%M",
                "%Y-%m-%d %H:%M:%S",
                "%Y-%m-%dT%H:%M:%S",
                "%Y%m%d%H%M",
            ]

            date_str = date_str.strip()
            for fmt in formats:
                try:
                    return datetime.strptime(date_str, fmt)
                except ValueError:
                    continue

            # "2024.01.30." í˜•ì‹ ì²˜ë¦¬
            if re.match(r"\d{4}\.\d{2}\.\d{2}\.", date_str):
                date_str = date_str.rstrip(".") + " 00:00"
                return datetime.strptime(date_str, "%Y.%m.%d %H:%M")

        except Exception as e:
            logger.debug(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ({date_str}): {e}")

        return None

    # ==========================================================================
    # ê¸°ì¡´ ë©”ì„œë“œ
    # ==========================================================================

    def to_dict(self, article: NewsArticle) -> Dict[str, Any]:
        """
        NewsArticleì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜

        Args:
            article: NewsArticle ê°ì²´

        Returns:
            ë”•ì…”ë„ˆë¦¬
        """
        return {
            "title": article.title,
            "content": article.content,
            "source": article.source,
            "url": article.url,
            "published_at": article.published_at.isoformat(),
            "ticker": article.ticker,
        }

    def to_dict_list(self, articles: List[NewsArticle]) -> List[Dict[str, Any]]:
        """
        NewsArticle ë¦¬ìŠ¤íŠ¸ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

        Args:
            articles: NewsArticle ë¦¬ìŠ¤íŠ¸

        Returns:
            ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        return [self.to_dict(article) for article in articles]
