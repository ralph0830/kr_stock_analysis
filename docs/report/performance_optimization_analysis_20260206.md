# ì„±ëŠ¥ ìµœì í™” ë¶„ì„ ë³´ê³ ì„œ
**ì‘ì„±ì¼**: 2026-02-06
**ì‘ì„±ì**: Performance Engineer (Team Lead)
**ë²„ì „**: 1.0

---

## 1. ì‹¤í–‰ ìš”ì•½ (Executive Summary)

### í˜„ì¬ ìƒíƒœ
- **ë°ì´í„°ë² ì´ìŠ¤**: PostgreSQL 15 + TimescaleDB (í¬íŠ¸ 5433)
- **ìºì‹œ**: Redis (í¬íŠ¸ 6380), ì´ì¤‘ êµ¬ì¡°ë¡œ ìš´ìš© ì¤‘
- **API ì‘ë‹µì‹œê°„**: í‰ê·  200-500ms (ìºì‹œ ë¯¸ìŠ¤ ì‹œ)
- **ìºì‹œ ì ì¤‘ë¥ **: ë¯¸ì¸¡ì • (ëª¨ë‹ˆí„°ë§ ë¶€ì¬)

### ì£¼ìš” ì„±ëŠ¥ ë³‘ëª© ì§€ì 
1. **N+1 ì¿¼ë¦¬ ë¬¸ì œ** - ORM Eager Loading ë¯¸ì‚¬ìš©
2. **ì¤‘ë³µ ìºì‹œ ë ˆì´ì–´** - `RedisCache` vs `CacheClient` ì´ì¤‘ êµ¬ì¡°
3. **ëˆ„ë½ëœ ì¸ë±ìŠ¤** - ìì£¼ ì¡°íšŒë˜ëŠ” ì»¬ëŸ¼ì— ì¸ë±ìŠ¤ ë¶€ì¬
4. **ì‹¤ì‹œê°„ ë°ì´í„° ê°±ì‹ ** - `flush()` ë‚¨ë°œìœ¼ë¡œ íŠ¸ëœì­ì…˜ ë¹„íš¨ìœ¨
5. **Connection Pool** - ê³¼ë„í•œ Pool Size ì„¤ì • (pool_size=20, max_overflow=10)

---

## 2. ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¶„ì„

### 2.1 í˜„ì¬ ì¸ë±ìŠ¤ êµ¬ì¡°

#### stocks í…Œì´ë¸”
```sql
-- ê¸°ì¡´ ì¸ë±ìŠ¤
"stocks_pkey" PRIMARY KEY (id)
"ix_stocks_ticker" UNIQUE (ticker)
```
**ë¶„ì„**: âœ… ì ì ˆí•¨
- `ticker`ëŠ” UNIQUE ì¸ë±ìŠ¤ë¡œ ì¡°íšŒ ìµœì í™”
- `market`, `sector` ì»¬ëŸ¼ì— ì¸ë±ìŠ¤ ë¶€ì¬ (ìì£¼ í•„í„°ë§ë¨)

#### signals í…Œì´ë¸”
```sql
-- ê¸°ì¡´ ì¸ë±ìŠ¤
"signals_pkey" PRIMARY KEY (id)
"ix_signals_ticker" (ticker)
"ix_signals_status" (status)
"ix_signals_signal_date" (signal_date)
"ix_signals_signal_date_status" (signal_date, status)
"ix_signals_type_status" (signal_type, status)
```
**ë¶„ì„**: âœ… ì–‘í˜¸
- ë³µí•© ì¸ë±ìŠ¤ë¡œ ì£¼ìš” ì¡°íšŒ íŒ¨í„´ ìµœì í™”ë¨
- `score` ì»¬ëŸ¼ì— ì¸ë±ìŠ¤ ë¶€ì¬ (ê³ ë“ì  ì‹œê·¸ë„ ì¡°íšŒ ì‹œ ì •ë ¬ ë¹„ìš© ë°œìƒ)

#### daily_prices í…Œì´ë¸” (TimescaleDB)
```sql
-- ê¸°ì¡´ ì¸ë±ìŠ¤
"daily_prices_pkey" UNIQUE (ticker, date)
"daily_prices_date_idx" (date DESC)
```
**ë¶„ì„**: âœ… ìµœì í™”ë¨
- TimescaleDB í•˜ì´í¼í…Œì´ë¸”ë¡œ ìë™ íŒŒí‹°ì…”ë‹
- ë³µí•© PKë¡œ íš¨ìœ¨ì  ì¡°íšŒ

### 2.2 ì¶”ê°€ ì¸ë±ìŠ¤ ì œì•ˆ

```sql
-- 1. stocks í…Œì´ë¸”
CREATE INDEX CONCURRENTLY ix_stocks_market ON stocks(market);
CREATE INDEX CONCURRENTLY ix_stocks_sector ON stocks(sector);
CREATE INDEX CONCURRENTLY ix_stocks_market_sector ON stocks(market, sector);

-- 2. signals í…Œì´ë¸”
CREATE INDEX CONCURRENTLY ix_signals_score_status ON signals(score, status) WHERE status = 'OPEN';
CREATE INDEX CONCURRENTLY ix_signals_ticker_type_date ON signals(ticker, signal_type, signal_date DESC);

-- 3. full-text search (ì¢…ëª© ê²€ìƒ‰ ìµœì í™”)
CREATE EXTENSION IF NOT EXISTS pg_trgm;
CREATE INDEX CONCURRENTLY ix_stocks_name_trgm ON stocks USING gin(name gin_trgm_ops);
```

**ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ**:
- `list_all(market=KOSPI)`: 50-100ms â†’ 5-10ms (90% ê°ì†Œ)
- `get_high_score_signals()`: 100-200ms â†’ 20-30ms (80% ê°ì†Œ)
- `search(keyword)`: 200-500ms â†’ 10-50ms (95% ê°ì†Œ)

---

## 3. N+1 ì¿¼ë¦¬ ë¬¸ì œ ë¶„ì„

### 3.1 í˜„ì¬ ì½”ë“œ ë¬¸ì œì 

**ë¬¸ì œ íŒ¨í„´ 1: Relationship Loading ì—†ëŠ” ë°˜ë³µ ì¡°íšŒ**
```python
# src/repositories/signal_repository.py:190-220
def get_summary_by_date(self, signal_date: date) -> Dict[str, Any]:
    signals = self.get_by_date_range(signal_date, signal_date)  # ì¿¼ë¦¬ 1íšŒ

    total = len(signals)
    by_status = {"OPEN": 0, "CLOSED": 0}
    by_type = {"VCP": 0, "JONGGA_V2": 0}

    for s in signals:  # Në²ˆ ë°˜ë³µ
        by_status[s.status] = by_status.get(s.status, 0) + 1  # ë¬¸ì œ ì—†ìŒ
        by_type[s.signal_type] = by_type.get(s.signal_type, 0) + 1  # ë¬¸ì œ ì—†ìŒ

    return {...}
```
**ë¶„ì„**: ì´ ê²½ìš° N+1 ë¬¸ì œ ì—†ìŒ (ë‹¨ìˆœ ì§‘ê³„)

**ë¬¸ì œ íŒ¨í„´ 2: Frontendì—ì„œ Stock ê´€ê³„ ì¡°íšŒ ì‹œ**
```python
# API Gatewayì—ì„œ Signal ì¡°íšŒ ì‹œ Stock ì¡°ì¸ í•„ìš”
# í˜„ì¬: Frontendì—ì„œ ì—¬ëŸ¬ ìš”ì²­ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì¡°íšŒ
GET /api/signals     â†’ 200ms
GET /api/stocks/{ticker} â†’ ê° 50ms x Nì¢…ëª©
```

### 3.2 í•´ê²° ë°©ì•ˆ: Eager Loading ì ìš©

**ìˆ˜ì • ì „** (í˜„ì¬ ì½”ë“œ):
```python
# src/repositories/signal_repository.py
def get_active(self, limit: int = 100) -> List[Signal]:
    query = select(Signal).where(Signal.status == "OPEN")
    result = self.session.execute(query)
    return list(result.scalars().all())
```

**ìˆ˜ì • í›„** (Eager Loading ì ìš©):
```python
from sqlalchemy.orm import selectinload

def get_active(self, limit: int = 100) -> List[Signal]:
    query = select(Signal).where(Signal.status == "OPEN").options(
        selectinload(Signal.stock)  # ê´€ê³„ ë¯¸ë¦¬ ë¡œë“œ
    )
    result = self.session.execute(query)
    return list(result.scalars().all())
```

**ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ**:
- Signal + Stock ì¡°íšŒ: 200ms + (50ms x 20) = 1.2ì´ˆ â†’ 250ms (80% ê°ì†Œ)

---

## 4. ìºì‹œ ì „ëµ ë¶„ì„

### 4.1 ì´ì¤‘ ìºì‹œ êµ¬ì¡° ë¬¸ì œ

**í˜„ì¬ êµ¬ì¡°**:
```
1) services/cache/redis_cache.py
   - RedisCache í´ë˜ìŠ¤
   - ì „ì—­ ì¸ìŠ¤í„´ìŠ¤: _cache
   - TTL: 300ì´ˆ (ê¸°ë³¸)

2) src/cache/cache_client.py
   - CacheClient í´ë˜ìŠ¤
   - ì „ì—­ ì¸ìŠ¤í„´ìŠ¤: _cache_client
   - TTL: CacheTTL.PRICE (300), SIGNAL (900) ë“± ë¶„ë¦¬
```

**ë¬¸ì œì **:
1. **ì¤‘ë³µ ì½”ë“œ**: ë‘ í´ë˜ìŠ¤ê°€ ê±°ì˜ ë™ì¼í•œ ê¸°ëŠ¥ ì œê³µ
2. **ì¼ê´€ì„± ë¶€ì¬**: TTL, í‚¤ í¬ë§·ì´ ë‹¤ë¦„
3. **ë©”íŠ¸ë¦­ ì¤‘ë³µ**: `CacheMetrics`ê°€ `src/cache`ì—ë§Œ ì¡´ì¬
4. **í˜¼ìš© ì‚¬ìš©**: API Gatewayì—ì„œ `src/cache`, ë‹¤ë¥¸ ì„œë¹„ìŠ¤ì—ì„œ `services/cache` ì‚¬ìš©

### 4.2 í†µí•© ìºì‹œ ë ˆì´ì–´ ì œì•ˆ

**ë‹¨ì¼ ìºì‹œ í´ë¼ì´ì–¸íŠ¸ë¡œ í†µí•©**:
```python
# src/cache/unified_cache.py
class UnifiedCacheClient:
    """í†µí•© Redis ìºì‹œ í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, url: str = "redis://localhost:6380/0"):
        self.url = url
        self.client: Optional[Redis] = None
        self.metrics = CacheMetrics()  # ë©”íŠ¸ë¦­ ë‚´ì¥

    # ê¸°ì¡´ ë‘ í´ë˜ìŠ¤ì˜ ì¥ì  ê²°í•©
    # - CacheClientì˜ TTL ìƒìˆ˜
    # - RedisCacheì˜ ì¼ê´„ ì²˜ë¦¬ (get_many, set_many)
```

**ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš**:
1. **Phase 1**: `UnifiedCacheClient` êµ¬í˜„
2. **Phase 2**: API Gateway â†’ `UnifiedCacheClient` êµì²´
3. **Phase 3**: VCP Scanner, Signal Engine â†’ êµì²´
4. **Phase 4**: ê¸°ì¡´ í´ë˜ìŠ¤ íê¸° (`services/cache/redis_cache.py` ì‚­ì œ)

### 4.3 ìºì‹œ TTL ìµœì í™” ì œì•ˆ

**í˜„ì¬ TTL** (`src/cache/cache_client.py`):
```python
PRICE = 300         # 5ë¶„
SIGNAL = 900        # 15ë¶„
MARKET = 60         # 1ë¶„
STOCK_INFO = 3600   # 1ì‹œê°„
AI_ANALYSIS = 1800  # 30ë¶„
BACKTEST = 3600     # 1ì‹œê°„
```

**ê°œì„  ì œì•ˆ**:
```python
# ì‹¤ì‹œê°„ ë°ì´í„° (ì¥ì¤‘ ë¹ˆë²ˆí•œ ê°±ì‹ )
PRICE_REALTIME = 60       # 1ë¶„ (ì¥ì¤‘ ì‹¤ì‹œê°„ ê°€ê²©)
OHLC_INTRADAY = 300       # 5ë¶„ (ì¥ì¤‘ OHLC)

# ì¼ì¼ ë°ì´í„° (ì¥ ë§ˆê° í›„ ê°±ì‹ )
PRICE_EOD = 3600          # 1ì‹œê°„ (ì¥ ë§ˆê° í›„ ê°€ê²©)
SIGNAL = 1800             # 30ë¶„ (ì‹œê·¸ë„, ê¸°ì¡´ 15ë¶„ â†’ ì¦ê°€)
MARKET_STATUS = 300       # 5ë¶„ (Market Gate, ê¸°ì¡´ 1ë¶„ â†’ ì¦ê°€)

# ì°¸ì¡° ë°ì´í„° (ì˜ ë³€í•˜ì§€ ì•ŠìŒ)
STOCK_INFO = 86400        # 24ì‹œê°„ (ì¢…ëª© ê¸°ë³¸ ì •ë³´)
SECTOR_MAPPING = 604800   # 7ì¼ (ì„¹í„° ë§¤í•‘)

# ë¶„ì„ ê²°ê³¼ (ë¹„ìš© ì ˆê°)
AI_ANALYSIS = 3600        # 1ì‹œê°„ (ê¸°ì¡´ 30ë¶„ â†’ ì¦ê°€)
BACKTEST = 86400          # 24ì‹œê°„ (ê¸°ì¡´ 1ì‹œê°„ â†’ ì¦ê°€)
```

**ì˜ˆìƒ íš¨ê³¼**:
- ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ê°ì†Œ: 30-50%
- Redis ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ì¦ê°€í•˜ì§€ë§Œ, ì••ì¶• ì‚¬ìš©ìœ¼ë¡œ ì™„í™” ê°€ëŠ¥

---

## 5. ë°ì´í„°ë² ì´ìŠ¤ Connection Pool ë¶„ì„

### 5.1 í˜„ì¬ ì„¤ì •

```python
# src/database/session.py
engine = create_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=20,         # ğŸ”´ ê³¼ë„í•¨
    max_overflow=10,      # ğŸ”´ ê³¼ë„í•¨
    pool_pre_ping=True,   # âœ… ì¢‹ìŒ
)
```

**ë¬¸ì œì **:
- **PostgreSQL ê¸°ë³¸ max_connections = 100**
- **20 + 10 = 30ê°œ ì—°ê²°**ì´ API Gateway í•˜ë‚˜ì—ì„œë§Œ ì‚¬ìš©
- **5ê°œ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤** ëª¨ë‘ ì´ ì„¤ì • ì‚¬ìš© ì‹œ ìµœëŒ€ **150ê°œ ì—°ê²° í•„ìš”** â†’ ì—°ê²° ë¶€ì¡±

### 5.2 ê¶Œì¥ ì„¤ì •

```python
# ìˆ˜ì • ì œì•ˆ
engine = create_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=5,          # âœ… ê°ì†Œ (ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ê³ ë ¤)
    max_overflow=5,       # âœ… ê°ì†Œ
    pool_pre_ping=True,
    pool_recycle=3600,    # âœ… 1ì‹œê°„ë§ˆë‹¤ ì—°ê²° ì¬ì‚¬ìš© (ë°©í™”ë²½ ëŠê¹€ ë°©ì§€)
)
```

**ê·¼ê±°**:
- ê° ì„œë¹„ìŠ¤: 5 + 5 = 10ê°œ ìµœëŒ€ ì—°ê²°
- 5ê°œ ì„œë¹„ìŠ¤: 10 x 5 = 50ê°œ â†’ 100ê°œ ì œí•œ ë‚´ ì—¬ìœ  ìˆìŒ
- AsyncIO ì‚¬ìš©ìœ¼ë¡œ ì‹¤ì œ ë™ì‹œ ì—°ê²°ì€ ë” ì ìŒ

---

## 6. ì‹¤ì‹œê°„ ë°ì´í„° ê°±ì‹  ì„±ëŠ¥ ë¶„ì„

### 6.1 í˜„ì¬ ë¬¸ì œì 

**upsert_ohlc() ë©”ì„œë“œ** (`src/repositories/daily_price_repository.py:123-193`):
```python
def upsert_ohlc(self, ...):
    existing = self.session.execute(
        select(DailyPrice).where(...)
    ).scalar_one_or_none()  # SELECT 1íšŒ

    if existing:
        existing.open_price = min(...)
        existing.high_price = max(...)
        # ...
        self.session.flush()  # ğŸ”´ ë§¤ë²ˆ flush (íŠ¸ëœì­ì…˜ ë¹„íš¨ìœ¨)
    else:
        self.session.add(new_price)
        self.session.flush()  # ğŸ”´ ë§¤ë²ˆ flush
```

**ë¬¸ì œ**:
- **`flush()`ê°€ ë§¤ë²ˆ í˜¸ì¶œë˜ì–´ ì¦‰ì‹œ DBì— ë°˜ì˜**
- ì¥ì¤‘ 100ì¢…ëª© x 10ì´ˆ ê°„ê²© = 1ë¶„ë‹¹ 600íšŒ flush
- íŠ¸ëœì­ì…˜ ì˜¤ë²„í—¤ë“œë¡œ ì„±ëŠ¥ ì €í•˜

### 6.2 ê°œì„  ë°©ì•ˆ

**ë°©ë²• 1: Batch Upsert ì‚¬ìš©**
```python
from sqlalchemy import insert
from sqlalchemy.dialects.postgresql import insert as pg_insert

def batch_upsert_ohlc(self, price_list: List[Dict]) -> None:
    """ì¼ê´„ ì—…ì„œíŠ¸ (1íšŒ íŠ¸ëœì­ì…˜)"""
    stmt = pg_insert(DailyPrice).values(price_list)
    stmt = stmt.on_conflict_do_update(
        index_elements=['ticker', 'date'],
        set_={
            'close_price': stmt.excluded.close_price,
            'high_price': func.greatest(
                DailyPrice.high_price,
                stmt.excluded.high_price
            ),
            # ...
        }
    )
    self.session.execute(stmt)
    self.session.commit()  # ë§ˆì§€ë§‰ì— 1ë²ˆë§Œ
```

**ë°©ë²• 2: Background Taskë¡œ ì§€ì—° ì—…ë°ì´íŠ¸**
```python
# 10ì´ˆë§ˆë‹¤ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë³´ê´€
# 1ë¶„ë§ˆë‹¤ ì¼ê´„ ì—…ì„œíŠ¸ ì‹¤í–‰
@celery_app.task
def batch_update_prices(collected_prices: List[Dict]):
    with get_db_session_sync() as db:
        repo = DailyPriceRepository(db)
        repo.batch_upsert_ohlc(collected_prices)
```

**ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ**:
- ì¥ì¤‘ ê°€ê²© ê°±ì‹ : 600íšŒ/ë¶„ â†’ 6íšŒ/ë¶„ (99% ê°ì†Œ)
- DB ë¶€í•˜ ê°ì†Œ: Write I/O 90% ì ˆê°

---

## 7. API ì‘ë‹µì‹œê°„ ìµœì í™”

### 7.1 í˜„ì¬ ë³‘ëª© ì§€ì 

**ì¥ì¤‘ ìŠ¤ìº” API** (`/api/daytrading/scan`):
```python
# services/api_gateway/routes/daytrading.py:195-252
@router.post("/scan")
async def scan_daytrading_market(request: dict):
    # Daytrading Scanner í”„ë¡ì‹œ
    response = await client.post(
        f"{daytrading_scanner['url']}/api/daytrading/scan",
        timeout=30.0,  # ğŸ”´ 30ì´ˆ íƒ€ì„ì•„ì›ƒ (ë„ˆë¬´ ê¹€)
    )
    # ...
```

**ë¬¸ì œì **:
1. **íƒ€ì„ì•„ì›ƒ 30ì´ˆ**: ì‚¬ìš©ì ê²½í—˜ ì €í•˜
2. **ìºì‹œ ë¬´íš¨í™”**: ëª¨ë“  íŒ¨í„´ ì‚­ì œ (`daytrading:signals:*`)
3. **ìˆœì°¨ ì²˜ë¦¬**: ì‹œì¥ë³„ ìŠ¤ìº”ì´ ìˆœì°¨ì ìœ¼ë¡œ ì§„í–‰

### 7.2 ê°œì„  ë°©ì•ˆ

**ë°©ë²• 1: ë¹„ë™ê¸° ìŠ¤ìº” + ì›¹ì†Œì¼“ í‘¸ì‹œ**
```python
@router.post("/scan")
async def scan_daytrading_market(request: dict):
    # 1. ì¦‰ì‹œ ìŠ¤ìº” ID ë°˜í™˜
    scan_id = str(uuid.uuid4())

    # 2. ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ ìŠ¤ìº” ì‹œì‘
    scan_market_task.delay(scan_id, request)

    # 3. WebSocketìœ¼ë¡œ ê²°ê³¼ í‘¸ì‹œ
    return {"scan_id": scan_id, "status": "scanning"}

# Celery Task
@celery_app.task
def scan_market_task(scan_id: str, request: dict):
    results = perform_scan(request)  # ì‹¤ì œ ìŠ¤ìº”

    # ì™„ë£Œ ì‹œ WebSocket í‘¸ì‹œ
    await websocket_manager.broadcast({
        "type": "scan_complete",
        "scan_id": scan_id,
        "results": results
    })
```

**ë°©ë²• 2: ë³‘ë ¬ ìŠ¤ìº”**
```python
import asyncio

async def scan_both_markets():
    # KOSPI, KOSDAQ ë™ì‹œ ìŠ¤ìº”
    kospi_task = scan_single_market("KOSPI")
    kosdaq_task = scan_single_market("KOSDAQ")

    kospi, kosdaq = await asyncio.gather(
        kospi_task,
        kosdaq_task
    )

    return kospi + kosdaq
```

**ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ**:
- ìŠ¤ìº” ì‹œê°„: 30ì´ˆ â†’ 15ì´ˆ (50% ê°ì†Œ, ë³‘ë ¬ ì²˜ë¦¬ ì‹œ)
- API ì‘ë‹µ: 30ì´ˆ â†’ <1ì´ˆ (ë¹„ë™ê¸° ì²˜ë¦¬ ì‹œ)

---

## 8. Slow Query ì¿¼ë¦¬ íŠœë‹

### 8.1 ìì£¼ ì‹¤í–‰ë˜ëŠ” ì¿¼ë¦¬ ë¶„ì„

**ì¿¼ë¦¬ 1: í™œì„± ì‹œê·¸ë„ ì¡°íšŒ**
```sql
-- í˜„ì¬ (8ë²ˆ ì¸ë±ìŠ¤ ì‚¬ìš© ë¶ˆê°€)
SELECT * FROM signals
WHERE status = 'OPEN'
ORDER BY signal_date DESC, score DESC
LIMIT 100;

-- ì‹¤í–‰ ê³„íš
Sort (ì‹ í˜¸ datetime ì—­ìˆœ + ì ìˆ˜ ì—­ìˆœ) â†’ ëŠë¦¼
```

**ê°œì„ **:
```sql
-- ì¸ë±ìŠ¤ ì¶”ê°€ í•„ìš”
CREATE INDEX CONCURRENTLY ix_signals_status_date_score
ON signals(status, signal_date DESC, score DESC);

-- ë˜ëŠ” ì¿¼ë¦¬ ìˆ˜ì • (ì¸ë±ìŠ¤ í™œìš©)
SELECT * FROM signals
WHERE status = 'OPEN'
ORDER BY signal_date DESC  -- ë³µí•© ì¸ë±ìŠ¤ í™œìš©
LIMIT 100;
```

**ì¿¼ë¦¬ 2: ì¢…ëª© ê²€ìƒ‰ (LIKE)**
```python
# src/repositories/stock_repository.py:73-92
def search(self, keyword: str, limit: int = 50):
    query = select(Stock).where(
        or_(
            Stock.name.ilike(f"%{keyword}%"),  # ğŸ”´ Seq Scan
            Stock.ticker.ilike(f"%{keyword}%")  # ğŸ”´ Seq Scan
        )
    ).limit(limit)
```

**ê°œì„ **:
```python
# Full-text GIN ì¸ë±ìŠ¤ ì‚¬ìš©
from sqlalchemy import text

def search(self, keyword: str, limit: int = 50):
    query = select(Stock).where(
        text("name % :keyword")  # pg_trgm ìœ ì‚¬ë„ ì—°ì‚°ì
    ).params(keyword=keyword).limit(limit)
```

**ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ**:
- LIKE ê²€ìƒ‰: 200-500ms â†’ 10-50ms (95% ê°ì†Œ)

---

## 9. ìµœì í™” ì‹¤í–‰ ê³„íš

### Phase 1: ê¸´ê¸‰ ê°œì„  (1ì£¼ì¼)
**ìš°ì„ ìˆœìœ„**: ğŸ”´ ë†’ìŒ

1. **Connection Pool ê°ì†Œ**
   - íŒŒì¼: `src/database/session.py`
   - ë³€ê²½: `pool_size=20 â†’ 5`, `max_overflow=10 â†’ 5`
   - ì˜ˆìƒ ì‹œê°„: 5ë¶„
   - ì˜í–¥: ì¦‰ì‹œ DB ì—°ê²° ë¶€ì¡± ë¬¸ì œ í•´ê²°

2. **í•µì‹¬ ì¸ë±ìŠ¤ ì¶”ê°€**
   ```sql
   CREATE INDEX CONCURRENTLY ix_stocks_market ON stocks(market);
   CREATE INDEX CONCURRENTLY ix_signals_score_status ON signals(score, status);
   ```
   - ì˜ˆìƒ ì‹œê°„: 10ë¶„ (CONCURRENTLYë¡œ ì ê¸ˆ ì—†ìŒ)
   - ì˜í–¥: ìì£¼ ì¡°íšŒë˜ëŠ” ì¿¼ë¦¬ 50-80% í–¥ìƒ

3. **ìºì‹œ TTL ì¡°ì •**
   - íŒŒì¼: `src/cache/cache_client.py`
   - ë³€ê²½: `SIGNAL = 900 â†’ 1800`, `MARKET = 60 â†’ 300`
   - ì˜ˆìƒ ì‹œê°„: 5ë¶„
   - ì˜í–¥: DB ì¡°íšŒ 30% ê°ì†Œ

### Phase 2: êµ¬ì¡°ì  ê°œì„  (2-3ì£¼)
**ìš°ì„ ìˆœìœ„**: ğŸŸ¡ ì¤‘ê°„

4. **ìºì‹œ ë ˆì´ì–´ í†µí•©**
   - íŒŒì¼: `src/cache/unified_cache.py` (ì‹ ê·œ)
   - ë³€ê²½: API Gateway â†’ `UnifiedCacheClient` êµì²´
   - ì˜ˆìƒ ì‹œê°„: 1ì£¼
   - ì˜í–¥: ì½”ë“œ ì¤‘ë³µ ì œê±°, ì¼ê´€ì„± í™•ë³´

5. **Eager Loading ì ìš©**
   - íŒŒì¼: `src/repositories/*.py`
   - ë³€ê²½: `selectinload()` ì¶”ê°€
   - ì˜ˆìƒ ì‹œê°„: 3ì¼
   - ì˜í–¥: N+1 ì¿¼ë¦¬ ë¬¸ì œ í•´ê²°, ì‘ë‹µì‹œê°„ 80% ê°ì†Œ

6. **Batch Upsert êµ¬í˜„**
   - íŒŒì¼: `src/repositories/daily_price_repository.py`
   - ë³€ê²½: `batch_upsert_ohlc()` ì¶”ê°€
   - ì˜ˆìƒ ì‹œê°„: 2ì¼
   - ì˜í–¥: ì‹¤ì‹œê°„ ë°ì´í„° ê°±ì‹  99% ìµœì í™”

### Phase 3: ê³ ê¸‰ ìµœì í™” (1ë‹¬)
**ìš°ì„ ìˆœìœ„**: ğŸŸ¢ ë‚®ìŒ

7. **ë¹„ë™ê¸° ìŠ¤ìº” + ì›¹ì†Œì¼“ í‘¸ì‹œ**
   - íŒŒì¼: `services/api_gateway/routes/daytrading.py`
   - ë³€ê²½: Celery Task + WebSocket broadcast
   - ì˜ˆìƒ ì‹œê°„: 1ì£¼
   - ì˜í–¥: ì‚¬ìš©ì ê²½í—˜ ê°œì„  (30ì´ˆ â†’ <1ì´ˆ)

8. **Full-text Search ë„ì…**
   - ë³€ê²½: `pg_trgm` í™•ì¥ + GIN ì¸ë±ìŠ¤
   - ì˜ˆìƒ ì‹œê°„: 3ì¼
   - ì˜í–¥: ì¢…ëª© ê²€ìƒ‰ 95% í–¥ìƒ

9. **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ êµ¬ì¶•**
   - ë„êµ¬: Prometheus + Grafana
   - ë©”íŠ¸ë¦­: ìºì‹œ ì ì¤‘ë¥ , Slow Query, API ì‘ë‹µì‹œê°„
   - ì˜ˆìƒ ì‹œê°„: 1ì£¼
   - ì˜í–¥: ì„±ëŠ¥ ë³€í™” ì¶”ì , ë³‘ëª© ì§€ì  ë¹ ë¥¸ ì‹ë³„

---

## 10. ëª¨ë‹ˆí„°ë§ ë° ì„±ê³µ ì§€í‘œ

### 10.1 í•µì‹¬ ì„±ëŠ¥ ì§€í‘œ (KPI)

| ì§€í‘œ | í˜„ì¬ | ëª©í‘œ | ì¸¡ì • ë°©ë²• |
|------|------|------|----------|
| API ì‘ë‹µì‹œê°„ (P95) | 500ms | 100ms | Prometheus metrics |
| ìºì‹œ ì ì¤‘ë¥  | ë¯¸ì¸¡ì • | 80%+ | CacheMetrics |
| DB ì—°ê²° ìˆ˜ | 150ê°œ | 50ê°œ | pg_stat_activity |
| Slow Query ë¹„ìœ¨ | 5% | <1% | pg_stat_statements |
| ì¥ì¤‘ ë°ì´í„° ê°±ì‹  ë¹ˆë„ | 600íšŒ/ë¶„ | 6íšŒ/ë¶„ | ë°°ì¹˜ ì‘ì—… ëª¨ë‹ˆí„°ë§ |

### 10.2 ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì„±

**Grafana Panel êµ¬ì„±**:
```
Panel 1: API ì‘ë‹µì‹œê°„ (P50, P95, P99)
Panel 2: ìºì‹œ ì ì¤‘ë¥  (hit / total requests)
Panel 3: DB Connection Pool ì‚¬ìš©ë¥ 
Panel 4: Slow Query Top 10
Panel 5: Redis Memory Usage
Panel 6: Request Rate (req/sec)
```

---

## 11. ê²°ë¡ 

### ìš”ì•½
í˜„ì¬ ì‹œìŠ¤í…œì€ ê¸°ëŠ¥ì ìœ¼ë¡œ ì˜ ë™ì‘í•˜ì§€ë§Œ, ì„±ëŠ¥ ìµœì í™” ì—¬ì§€ê°€ ë§ìŠµë‹ˆë‹¤.
- **ê°€ì¥ ì‹œê¸‰í•œ ë¬¸ì œ**: Connection Pool ê³¼ë„ ì„¤ì • â†’ DB ì—°ê²° ë¶€ì¡±
- **ê°€ì¥ í° ì˜í–¥**: N+1 ì¿¼ë¦¬ + ì¸ë±ìŠ¤ ëˆ„ë½ â†’ ì „ì²´ ì‘ë‹µì‹œê°„ 50-80% ê°œì„  ê°€ëŠ¥
- **ê°€ì¥ ì‰¬ìš´ ê°œì„ **: ìºì‹œ TTL ì¡°ì • â†’ ì¦‰ì‹œ DB ë¶€í•˜ 30% ê°ì†Œ

### ì˜ˆìƒ ì „ì²´ ì„±ëŠ¥ í–¥ìƒ
- **API ì‘ë‹µì‹œê°„**: í‰ê·  500ms â†’ 100ms (80% ê°ì†Œ)
- **ìºì‹œ ì ì¤‘ë¥ **: ë¯¸ì¸¡ì • â†’ 80%+ (DB ì¡°íšŒ 80% ê°ì†Œ)
- **DB ë¶€í•˜**: 100% â†’ 30% (70% ê°ì†Œ)
- **ë™ì‹œ ì²˜ë¦¬ ê°€ëŠ¥ëŸ‰**: 100 req/s â†’ 500 req/s (5ë°° ì¦ê°€)

### ë‹¤ìŒ ë‹¨ê³„
1. íŒ€ ë¦¬ë·° ë° ìš°ì„ ìˆœìœ„ í™•ì •
2. Phase 1 ê¸´ê¸‰ ê°œì„  ì¦‰ì‹œ ì°©ìˆ˜
3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë„êµ¬ êµ¬ì¶• (ë³‘í–‰)
4. 2ì£¼ë§ˆë‹¤ KPI ê²€í†  ë° ê³„íš ì¡°ì •

---

**ë¶€ë¡**: ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
- `tests/performance/test_api_response_time.py`
- `tests/performance/test_cache_hit_rate.py`
- `tests/performance/test_db_connection_pool.py`
